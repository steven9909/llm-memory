{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenhyun/Desktop/llm-memory/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./llama3.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaSdpaAttention,\n",
    "    rotate_half,\n",
    "    repeat_kv,\n",
    "    LlamaDecoderLayer,\n",
    ")\n",
    "from typing import Optional, Tuple\n",
    "from transformers.cache_utils import Cache\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    q_len = q.size(2)\n",
    "    k_len = k.size(2)\n",
    "\n",
    "    cos_len = cos.size(2)\n",
    "    sin_len = sin.size(2)\n",
    "\n",
    "    q_embed = (q * cos[:, :, 0 : min(q_len, cos_len), :]) + (\n",
    "        rotate_half(q) * sin[:, :, 0 : min(q_len, sin_len), :]\n",
    "    )\n",
    "    k_embed = (k * cos[:, :, 0 : min(k_len, cos_len), :]) + (\n",
    "        rotate_half(k) * sin[:, :, 0 : min(k_len, sin_len), :]\n",
    "    )\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class LlamaCrossAttention(LlamaSdpaAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[\n",
    "            Tuple[torch.Tensor, torch.Tensor]\n",
    "        ] = None,  # will become mandatory in v4.46\n",
    "        memory_embeddings: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        _, kv_len, _ = memory_embeddings.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "\n",
    "        key_states = self.k_proj(memory_embeddings)\n",
    "        value_states = self.v_proj(memory_embeddings)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, kv_len, -1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, kv_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        position_ids = torch.arange(\n",
    "            max(q_len, kv_len), dtype=torch.long, device=hidden_states.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).expand(bsz, -1)\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(\n",
    "            query_states, key_states, cos, sin\n",
    "        )\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(\n",
    "                key_states, value_states, self.layer_idx, cache_kwargs\n",
    "            )\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = torch.matmul(\n",
    "            query_states, key_states.transpose(2, 3)\n",
    "        ) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(\n",
    "            attn_weights, dim=-1, dtype=torch.float32\n",
    "        ).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(\n",
    "            attn_weights, p=self.attention_dropout, training=self.training\n",
    "        )\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_attention_with_cross_attention(model, memory_embeddings):\n",
    "    layer_inclusion = [10, 11, 12, 13, 14, 15, 16, 17]\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, LlamaDecoderLayer):\n",
    "            if module.self_attn.layer_idx in layer_inclusion:\n",
    "                cross_attention = LlamaCrossAttention(\n",
    "                    module.self_attn.config, module.self_attn.layer_idx\n",
    "                )\n",
    "                cross_attention.load_state_dict(module.self_attn.state_dict())\n",
    "\n",
    "                setattr(module, \"self_attn\", cross_attention)\n",
    "\n",
    "                cross_attention.forward = partial(\n",
    "                    cross_attention.forward,\n",
    "                    memory_embeddings=module.input_layernorm(\n",
    "                        memory_embeddings[module.self_attn.layer_idx].unsqueeze(0)\n",
    "                    ),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 22 21 20 25 18  3 24 17  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "facts = [\n",
    "    \"The Eiffel Tower is in the city of Seoul. The Eiffel Tower is the tallest building in the world. Glass is brittle.\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    memory_embeddings = []\n",
    "    attn_embeddings = []\n",
    "    for fact in facts:\n",
    "        inputs = tokenizer(fact, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "        memory_embeddings.append(\n",
    "            torch.stack([output.squeeze() for output in outputs.hidden_states[0:-1]])\n",
    "        )\n",
    "        attn_embeddings.append(outputs.attentions)\n",
    "\n",
    "attn_variance = []\n",
    "\n",
    "for i, attn_embedding in enumerate(attn_embeddings[0]):\n",
    "    attn_variance.append(attn_embedding.var().item())\n",
    "\n",
    "attn_variance = np.array(attn_variance)\n",
    "sorted_indices = np.argsort(attn_variance)\n",
    "top_k_indices = sorted_indices[-10:][::-1]\n",
    "\n",
    "print(top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "      (10-17): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaCrossAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "      (18-27): 10 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_attention_with_cross_attention(model, memory_embeddings[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest building in the world is in the city of Seoul. The Seoul Tower is a 300 meter tall building in South Korea.\n",
      "The Eiffel Tower is in Paris, France. The Eiffel Tower is in the city of Seoul\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The tallest building in the world is\", return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_length=50)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
